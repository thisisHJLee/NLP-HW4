{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드 및 전처리\n",
    "# 필요한 도구들 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import shutil\n",
    "import os\n",
    "import unicodedata\n",
    "import urllib3\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "http = urllib3.PoolManager()\n",
    "url ='http://www.manythings.org/anki/fra-eng.zip'\n",
    "filename = 'fra-eng.zip'\n",
    "path = os.getcwd()\n",
    "zipfilename = os.path.join(path, filename)\n",
    "with http.request('GET', url, preload_content=False) as r, open(zipfilename, 'wb') as out_file:       \n",
    "    shutil.copyfileobj(r, out_file)\n",
    "\n",
    "with zipfile.ZipFile(zipfilename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 33000 # 사용할 샘플의 개수(33000개) 변수에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수들을 구현\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(sent):\n",
    "    # 위에서 구현한 함수를 내부적으로 호출\n",
    "    sent = unicode_to_ascii(sent.lower())\n",
    "\n",
    "    # 단어와 구두점 사이에 공백을 만듭니다.\n",
    "    # Ex) \"he is a boy.\" => \"he is a boy .\"\n",
    "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
    "\n",
    "    # (a-z, A-Z, \".\", \"?\", \"!\", \",\") 이들을 제외하고는 전부 공백으로 변환합니다.\n",
    "    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n",
    "\n",
    "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have you had dinner ?\n",
      "b'avez vous deja dine ?'\n"
     ]
    }
   ],
   "source": [
    "# 전처리 테스트\n",
    "en_sent = u\"Have you had dinner?\"\n",
    "fr_sent = u\"Avez-vous déjà diné?\"\n",
    "print(preprocess_sentence(en_sent))\n",
    "print(preprocess_sentence(fr_sent).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "전체 데이터에서 33,000개의 샘플만 불러오되, 모든 전처리를 수행하는 함수를 만듭니다.\n",
    "또한 훈련 과정에서 교사 강요(Teacher Forcing)을 사용할 예정이므로,\n",
    "훈련 시 사용할 디코더의 입력 시퀀스와 실제값에 해당되는 출력 시퀀스를 따로 분리하여 저장합니다.\n",
    "입력 시퀀스에는 시작을 의미하는 토큰인 <sos>를 추가하고, 출력 시퀀스에는 종료를 의미하는 토큰인 <eos>를 추가합니다.\n",
    "'''\n",
    "def load_preprocessed_data():\n",
    "    encoder_input, decoder_input, decoder_target = [], [], []\n",
    "\n",
    "    with open(\"fra.txt\", \"r\", encoding='utf8') as lines:\n",
    "        for i, line in enumerate(lines):\n",
    "\n",
    "            # source 데이터와 target 데이터 분리\n",
    "            src_line, tar_line, _ = line.strip().split('\\t')\n",
    "\n",
    "            # source 데이터 전처리\n",
    "            src_line_input = [w for w in preprocess_sentence(src_line).split()]\n",
    "\n",
    "            # target 데이터 전처리\n",
    "            tar_line = preprocess_sentence(tar_line)\n",
    "            tar_line_input = [w for w in (\"<sos> \" + tar_line).split()]\n",
    "            tar_line_target = [w for w in (tar_line + \" <eos>\").split()]\n",
    "\n",
    "            encoder_input.append(src_line_input)\n",
    "            decoder_input.append(tar_line_input)\n",
    "            decoder_target.append(tar_line_target)\n",
    "\n",
    "            if i == num_samples - 1:\n",
    "                break\n",
    "\n",
    "    return encoder_input, decoder_input, decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', '.'], ['hi', '.'], ['hi', '.'], ['run', '!'], ['run', '!']]\n",
      "[['<sos>', 'va', '!'], ['<sos>', 'salut', '!'], ['<sos>', 'salut', '.'], ['<sos>', 'cours', '!'], ['<sos>', 'courez', '!']]\n",
      "[['va', '!', '<eos>'], ['salut', '!', '<eos>'], ['salut', '.', '<eos>'], ['cours', '!', '<eos>'], ['courez', '!', '<eos>']]\n"
     ]
    }
   ],
   "source": [
    "# 얻은 3개의 데이터셋인  인코더의 입력, 디코더의 입력, 디코더의 실제값을 상위 5개 샘플만 출력해보기\n",
    "sents_en_in, sents_fra_in, sents_fra_out = load_preprocessed_data()\n",
    "print(sents_en_in[:5])\n",
    "print(sents_fra_in[:5])\n",
    "print(sents_fra_out[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 토크나이저를 통해 단어 집합을 생성하고, 텍스트 시퀀스를 정수 시퀀스로 변환하는 정수 인코딩 과정을 수행\n",
    "tokenizer_en = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_en.fit_on_texts(sents_en_in)\n",
    "encoder_input = tokenizer_en.texts_to_sequences(sents_en_in)\n",
    "\n",
    "tokenizer_fra = Tokenizer(filters=\"\", lower=False)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_in)\n",
    "tokenizer_fra.fit_on_texts(sents_fra_out)\n",
    "decoder_input = tokenizer_fra.texts_to_sequences(sents_fra_in)\n",
    "decoder_target = tokenizer_fra.texts_to_sequences(sents_fra_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 수행\n",
    "encoder_input = pad_sequences(encoder_input, padding=\"post\")\n",
    "decoder_input = pad_sequences(decoder_input, padding=\"post\")\n",
    "decoder_target = pad_sequences(decoder_target, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 8)\n",
      "(33000, 16)\n",
      "(33000, 16)\n"
     ]
    }
   ],
   "source": [
    "# 얻은 데이터의 크기(shape)을 확인\n",
    "print(encoder_input.shape)\n",
    "print(decoder_input.shape)\n",
    "print(decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어 집합의 크기 : 4678, 프랑스어 단어 집합의 크기 : 8032\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합의 크기를 정의\n",
    "src_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "tar_vocab_size = len(tokenizer_fra.word_index) + 1\n",
    "print(\"영어 단어 집합의 크기 : {:d}, 프랑스어 단어 집합의 크기 : {:d}\".format(src_vocab_size, tar_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "단어로부터 정수를 얻는 딕셔너리와 정수로부터 단어를 얻는 딕셔너리를 각각 만들어줍니다.\n",
    "이들은 훈련을 마치고 예측 과정과 실제값과 결과를 비교하는 경우에 사용됩니다.\n",
    "'''\n",
    "src_to_index = tokenizer_en.word_index\n",
    "index_to_src = tokenizer_en.index_word # 훈련 후 결과 비교할 때 사용\n",
    "\n",
    "tar_to_index = tokenizer_fra.word_index # 훈련 후 예측 과정에서 사용\n",
    "index_to_tar = tokenizer_fra.index_word # 훈련 후 결과 비교할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19764 17230  6028 ... 15899  3552 28968]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터를 분리하기 전에, 적절한 분포를 갖도록 데이터를 섞어주는 과정을 진행\n",
    "# 이를 위해 우선 순서가 섞인 정수 시퀀스 리스트를 만든다.\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이를 데이터셋의 순서로 지정\n",
    "# 샘플들이 기존 순서와 다른 순서로 섞이게 됨\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 39, 16, 46,  1,  0,  0,  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 임의로 30,997번째 샘플을 출력\n",
    "encoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  11,  50, 619,  22,   1,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11,  50, 619,  22,   1,   3,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target[30997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3300\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터의 10%를 테스트 데이터로 분리\n",
    "n_of_val = int(33000*0.1)\n",
    "print(n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29700, 8)\n",
      "(29700, 16)\n",
      "(29700, 16)\n",
      "(3300, 8)\n",
      "(3300, 16)\n",
      "(3300, 16)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print(encoder_input_train.shape) # 훈련 데이터의 샘플은 29,700개\n",
    "print(decoder_input_train.shape)\n",
    "print(decoder_target_train.shape)\n",
    "print(encoder_input_test.shape)  # 테스트 데이터의 샘플은 3,300개\n",
    "print(decoder_input_test.shape)\n",
    "print(decoder_target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기계 번역기 만들기\n",
    "# 모델 설계를 위해 필요한 도구들을 임포트\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# 임베딩 벡터와 LSTM의 은닉 상태의 크기를 특정 크기로 고정. 여기서는 50.\n",
    "latent_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "# Masking은 패딩 토큰인 숫자 0의 경우에는 연산을 제외하는 역할을 수행\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb =  Embedding(src_vocab_size, latent_dim)(encoder_inputs) # 임베딩 층\n",
    "enc_masking = Masking(mask_value=0.0)(enc_emb) # 패딩 0은 연산에서 제외\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True) # 상태값 리턴을 위해 return_state는 True\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_masking) # 은닉 상태와 셀 상태를 리턴\n",
    "encoder_states = [state_h, state_c] # 인코더의 은닉 상태와 셀 상태를 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(tar_vocab_size, latent_dim) # 임베딩 층\n",
    "dec_emb = dec_emb_layer(decoder_inputs) # 패딩 0은 연산에서 제외\n",
    "dec_masking = Masking(mask_value=0.0)(dec_emb)\n",
    "\n",
    "# 상태값 리턴을 위해 return_state는 True, 모든 시점에 대해서 단어를 예측하기 위해 return_sequences는 True\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "\n",
    "# 인코더의 은닉 상태를 초기 은닉 상태(initial_state)로 사용\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_masking,\n",
    "                                     initial_state=encoder_states)\n",
    "\n",
    "# 모든 시점의 결과에 대해서 소프트맥스 함수를 사용한 출력층을 통해 단어 예측\n",
    "decoder_dense = Dense(tar_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 50)     233900      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     401600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 50)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, None, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 50), (None,  20200       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 50), ( 20200       masking_1[0][0]                  \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 8032)   409632      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 1,085,532\n",
      "Trainable params: 1,085,532\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델의 입출력을 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# 모델의 파라미터 확인\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "233/233 [==============================] - 141s 604ms/step - loss: 3.1584 - acc: 0.6174 - val_loss: 1.8745 - val_acc: 0.7142\n",
      "Epoch 2/50\n",
      "233/233 [==============================] - 137s 590ms/step - loss: 1.6939 - acc: 0.7328 - val_loss: 1.6115 - val_acc: 0.7396\n",
      "Epoch 3/50\n",
      "233/233 [==============================] - 132s 566ms/step - loss: 1.5157 - acc: 0.7519 - val_loss: 1.4910 - val_acc: 0.7572\n",
      "Epoch 4/50\n",
      "233/233 [==============================] - 124s 534ms/step - loss: 1.4126 - acc: 0.7719 - val_loss: 1.4230 - val_acc: 0.7755\n",
      "Epoch 5/50\n",
      "233/233 [==============================] - 125s 538ms/step - loss: 1.3367 - acc: 0.7844 - val_loss: 1.3620 - val_acc: 0.7847\n",
      "Epoch 6/50\n",
      "233/233 [==============================] - 125s 537ms/step - loss: 1.2752 - acc: 0.7938 - val_loss: 1.3104 - val_acc: 0.7902\n",
      "Epoch 7/50\n",
      "233/233 [==============================] - 131s 564ms/step - loss: 1.2247 - acc: 0.8011 - val_loss: 1.2709 - val_acc: 0.7958\n",
      "Epoch 8/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 1.1819 - acc: 0.8069 - val_loss: 1.2404 - val_acc: 0.8001\n",
      "Epoch 9/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 1.1440 - acc: 0.8126 - val_loss: 1.2114 - val_acc: 0.8090\n",
      "Epoch 10/50\n",
      "233/233 [==============================] - 125s 534ms/step - loss: 1.1112 - acc: 0.8177 - val_loss: 1.1752 - val_acc: 0.8116\n",
      "Epoch 11/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 1.0819 - acc: 0.8217 - val_loss: 1.1456 - val_acc: 0.8167\n",
      "Epoch 12/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 1.0554 - acc: 0.8254 - val_loss: 1.1250 - val_acc: 0.8198\n",
      "Epoch 13/50\n",
      "233/233 [==============================] - 125s 537ms/step - loss: 1.0314 - acc: 0.8288 - val_loss: 1.1419 - val_acc: 0.8174\n",
      "Epoch 14/50\n",
      "233/233 [==============================] - 125s 538ms/step - loss: 1.0096 - acc: 0.8321 - val_loss: 1.0843 - val_acc: 0.8259\n",
      "Epoch 15/50\n",
      "233/233 [==============================] - 125s 537ms/step - loss: 0.9888 - acc: 0.8345 - val_loss: 1.0800 - val_acc: 0.8252\n",
      "Epoch 16/50\n",
      "233/233 [==============================] - 125s 537ms/step - loss: 0.9696 - acc: 0.8372 - val_loss: 1.0531 - val_acc: 0.8297\n",
      "Epoch 17/50\n",
      "233/233 [==============================] - 125s 536ms/step - loss: 0.9512 - acc: 0.8395 - val_loss: 1.0527 - val_acc: 0.8271\n",
      "Epoch 18/50\n",
      "233/233 [==============================] - 127s 543ms/step - loss: 0.9341 - acc: 0.8415 - val_loss: 1.0354 - val_acc: 0.8311\n",
      "Epoch 19/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 0.9178 - acc: 0.8436 - val_loss: 1.0296 - val_acc: 0.8304\n",
      "Epoch 20/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 0.9022 - acc: 0.8455 - val_loss: 1.0201 - val_acc: 0.8331\n",
      "Epoch 21/50\n",
      "233/233 [==============================] - 126s 539ms/step - loss: 0.8866 - acc: 0.8473 - val_loss: 1.0058 - val_acc: 0.8334\n",
      "Epoch 22/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 0.8721 - acc: 0.8492 - val_loss: 1.0155 - val_acc: 0.8322\n",
      "Epoch 23/50\n",
      "233/233 [==============================] - 128s 550ms/step - loss: 0.8591 - acc: 0.8508 - val_loss: 0.9970 - val_acc: 0.8360\n",
      "Epoch 24/50\n",
      "233/233 [==============================] - 128s 550ms/step - loss: 0.8474 - acc: 0.8528 - val_loss: 0.9822 - val_acc: 0.8377\n",
      "Epoch 25/50\n",
      "233/233 [==============================] - 126s 540ms/step - loss: 0.8359 - acc: 0.8544 - val_loss: 0.9711 - val_acc: 0.8407\n",
      "Epoch 26/50\n",
      "233/233 [==============================] - 126s 540ms/step - loss: 0.8247 - acc: 0.8561 - val_loss: 0.9676 - val_acc: 0.8408\n",
      "Epoch 27/50\n",
      "233/233 [==============================] - 126s 540ms/step - loss: 0.8136 - acc: 0.8578 - val_loss: 0.9627 - val_acc: 0.8411\n",
      "Epoch 28/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 0.8030 - acc: 0.8592 - val_loss: 0.9499 - val_acc: 0.8431\n",
      "Epoch 29/50\n",
      "233/233 [==============================] - 126s 540ms/step - loss: 0.7922 - acc: 0.8611 - val_loss: 0.9459 - val_acc: 0.8429\n",
      "Epoch 30/50\n",
      "233/233 [==============================] - 127s 543ms/step - loss: 0.7824 - acc: 0.8626 - val_loss: 0.9336 - val_acc: 0.8448\n",
      "Epoch 31/50\n",
      "233/233 [==============================] - 131s 561ms/step - loss: 0.7734 - acc: 0.8645 - val_loss: 0.9286 - val_acc: 0.8452\n",
      "Epoch 32/50\n",
      "233/233 [==============================] - 126s 541ms/step - loss: 0.7653 - acc: 0.8656 - val_loss: 0.9331 - val_acc: 0.8451\n",
      "Epoch 33/50\n",
      "233/233 [==============================] - 129s 554ms/step - loss: 0.7578 - acc: 0.8671 - val_loss: 0.9273 - val_acc: 0.8467\n",
      "Epoch 34/50\n",
      "233/233 [==============================] - 125s 537ms/step - loss: 0.7507 - acc: 0.8686 - val_loss: 0.9377 - val_acc: 0.8444\n",
      "Epoch 35/50\n",
      "233/233 [==============================] - 125s 534ms/step - loss: 0.7441 - acc: 0.8701 - val_loss: 0.9184 - val_acc: 0.8474\n",
      "Epoch 36/50\n",
      "233/233 [==============================] - 127s 547ms/step - loss: 0.7375 - acc: 0.8714 - val_loss: 0.9204 - val_acc: 0.8472\n",
      "Epoch 37/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 0.7308 - acc: 0.8729 - val_loss: 0.9125 - val_acc: 0.8504\n",
      "Epoch 38/50\n",
      "233/233 [==============================] - 125s 534ms/step - loss: 0.7244 - acc: 0.8741 - val_loss: 0.9297 - val_acc: 0.8461\n",
      "Epoch 39/50\n",
      "233/233 [==============================] - 124s 533ms/step - loss: 0.7182 - acc: 0.8753 - val_loss: 0.9151 - val_acc: 0.8497\n",
      "Epoch 40/50\n",
      "233/233 [==============================] - 124s 533ms/step - loss: 0.7121 - acc: 0.8766 - val_loss: 0.9036 - val_acc: 0.8522\n",
      "Epoch 41/50\n",
      "233/233 [==============================] - 124s 533ms/step - loss: 0.7060 - acc: 0.8778 - val_loss: 0.9033 - val_acc: 0.8520\n",
      "Epoch 42/50\n",
      "233/233 [==============================] - 124s 532ms/step - loss: 0.6995 - acc: 0.8790 - val_loss: 0.8998 - val_acc: 0.8526\n",
      "Epoch 43/50\n",
      "233/233 [==============================] - 124s 533ms/step - loss: 0.6932 - acc: 0.8802 - val_loss: 0.9003 - val_acc: 0.8517\n",
      "Epoch 44/50\n",
      "233/233 [==============================] - 124s 533ms/step - loss: 0.6863 - acc: 0.8812 - val_loss: 0.8879 - val_acc: 0.8542\n",
      "Epoch 45/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 0.6790 - acc: 0.8823 - val_loss: 0.8865 - val_acc: 0.8538\n",
      "Epoch 46/50\n",
      "233/233 [==============================] - 125s 535ms/step - loss: 0.6721 - acc: 0.8834 - val_loss: 0.8835 - val_acc: 0.8550\n",
      "Epoch 47/50\n",
      "233/233 [==============================] - 124s 532ms/step - loss: 0.6659 - acc: 0.8846 - val_loss: 0.8823 - val_acc: 0.8542\n",
      "Epoch 48/50\n",
      "233/233 [==============================] - 124s 534ms/step - loss: 0.6607 - acc: 0.8855 - val_loss: 0.8898 - val_acc: 0.8534\n",
      "Epoch 49/50\n",
      "233/233 [==============================] - 126s 540ms/step - loss: 0.6560 - acc: 0.8865 - val_loss: 0.8832 - val_acc: 0.8551\n",
      "Epoch 50/50\n",
      "233/233 [==============================] - 131s 560ms/step - loss: 0.6515 - acc: 0.8875 - val_loss: 0.8855 - val_acc: 0.8542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21d8da7e400>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(x = [encoder_input_train, decoder_input_train], y = decoder_target_train, \\\n",
    "          validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "          batch_size = 128, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 기계 번역기 동작시키기\n",
    "'''\n",
    "seq2seq는 훈련 과정과 테스트 과정에서의 동작 방식이 다릅니다.\n",
    "그래서 테스트 과정을 위해 모델을 다시 설계해주어야 합니다. \n",
    "'''\n",
    "\n",
    "# 인코더 설계\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# 디코더 설계\n",
    "# 이전 시점의 상태를 보관할 텐서\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# 훈련 때 사용했던 임베딩 층을 재사용\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 다음 단어 예측을 위해 이전 시점의 상태를 현 시점의 초기 상태로 사용\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "# 모든 시점에 대해서 단어 예측\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "# 디코더 정의\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 과정에서의 동작을 위한 함수 구현\n",
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 정수 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_to_index['<sos>']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    # stop_condition이 True가 될 때까지 루프 반복\n",
    "    # 구현의 간소화를 위해서 이 함수는 배치 크기를 1로 가정합니다.\n",
    "    while not stop_condition:\n",
    "        # 이점 시점의 상태 states_value를 현 시점의 초기 상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # 예측 결과를 단어로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = index_to_tar[sampled_token_index]\n",
    "\n",
    "         # 현재 시점의 예측 단어를 예측 문장에 추가\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # <eos>에 도달하거나 정해진 길이를 넘으면 중단.\n",
    "        if (sampled_char == '<eos>' or\n",
    "           len(decoded_sentence) > 50):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 확인을 위한 함수 정의\n",
    "def seq2src(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            temp = temp + index_to_src[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 번역문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2tar(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=tar_to_index['<sos>']) and i!=tar_to_index['<eos>']):\n",
    "            temp = temp + index_to_tar[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  i paid the bill . \n",
      "번역문 : j ai paye l addition . \n",
      "예측문 :  j ai pris le chien . \n",
      "\n",
      "\n",
      "원문 :  i m bilingual . \n",
      "번역문 : je suis bilingue . \n",
      "예측문 :  je suis une fois . \n",
      "\n",
      "\n",
      "원문 :  please get tom . \n",
      "번역문 : allez chercher tom s il vous plait . \n",
      "예측문 :  veuillez faire tout ou tom . \n",
      "\n",
      "\n",
      "원문 :  we re ready now . \n",
      "번역문 : nous sommes prets desormais . \n",
      "예측문 :  nous sommes desormais pretes . \n",
      "\n",
      "\n",
      "원문 :  that s a lemon tree . \n",
      "번역문 : c est un citronnier . \n",
      "예측문 :  c est un bon j . \n",
      "\n",
      "\n",
      "원문 :  are you envious ? \n",
      "번역문 : etes vous jalouse ? \n",
      "예측문 :  etes vous des enfants ? \n",
      "\n",
      "\n",
      "원문 :  are you ambitious ? \n",
      "번역문 : etes vous ambitieuse ? \n",
      "예측문 :  es tu seul ? \n",
      "\n",
      "\n",
      "원문 :  i like what you did . \n",
      "번역문 : j apprecie ce que vous avez fait . \n",
      "예측문 :  tu sais que tu as tort . \n",
      "\n",
      "\n",
      "원문 :  i have no choice . \n",
      "번역문 : je n ai pas le choix . \n",
      "예측문 :  je n ai pas le choix . \n",
      "\n",
      "\n",
      "원문 :  is tom lazy ? \n",
      "번역문 : est ce que tom est paresseux ? \n",
      "예측문 :  est ce que tom est libre ? \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력\n",
    "for seq_index in [176, 276, 376, 476, 576, 676, 776, 876, 976, 1076]:\n",
    "  input_seq = encoder_input_train[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_train[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_train[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  it was clean . \n",
      "번역문 : c etait propre . \n",
      "예측문 :  c etait de l amour . \n",
      "\n",
      "\n",
      "원문 :  that was unexpected . \n",
      "번역문 : ce n etait pas prevu . \n",
      "예측문 :  c etait de l amour . \n",
      "\n",
      "\n",
      "원문 :  i m buying . \n",
      "번역문 : c est moi qui paie . \n",
      "예측문 :  je vais ferai . \n",
      "\n",
      "\n",
      "원문 :  just wait a second . \n",
      "번역문 : attends juste une seconde . \n",
      "예측문 :  attendez une seconde ! \n",
      "\n",
      "\n",
      "원문 :  can i eat this cake ? \n",
      "번역문 : puis je manger ce gateau ? \n",
      "예측문 :  puis je manger ce livre ? \n",
      "\n",
      "\n",
      "원문 :  does he have money ? \n",
      "번역문 : a t il de l argent ? \n",
      "예측문 :  est ce que elle a l qu il sont ? \n",
      "\n",
      "\n",
      "원문 :  stop harassing me . \n",
      "번역문 : arretez de me harceler . \n",
      "예측문 :  arrete de me fut yeux ! \n",
      "\n",
      "\n",
      "원문 :  he ll get over it . \n",
      "번역문 : il s en remettra . \n",
      "예측문 :  il va vous vie . \n",
      "\n",
      "\n",
      "원문 :  fill in the blanks . \n",
      "번역문 : remplissez les blancs . \n",
      "예측문 :  les amour . \n",
      "\n",
      "\n",
      "원문 :  it s booby trapped . \n",
      "번역문 : c est piege . \n",
      "예측문 :  c est une chanson . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대해서 임의로 선택한 인덱스의 샘플의 결과를 출력\n",
    "for seq_index in [176, 276, 376, 476, 576, 676, 776, 876, 976, 1076]:\n",
    "  input_seq = encoder_input_test[seq_index: seq_index + 1]\n",
    "  decoded_sentence = decode_sequence(input_seq)\n",
    "\n",
    "  print(\"원문 : \",seq2src(encoder_input_test[seq_index]))\n",
    "  print(\"번역문 :\",seq2tar(decoder_input_test[seq_index]))\n",
    "  print(\"예측문 :\",decoded_sentence[:-5])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
